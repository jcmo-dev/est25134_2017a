---
title: 'Sesion 20: Arboles de decision (*Bagging*, *Random Forests*, *Boosting*)'
author: 'Juan Carlos Martinez-Ovando'
date: "Primavera 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

-------------------------------------------------------------

-------------------------------------------------------------

### Resumen y objetivo

 - En esta sesion revisaremos los modelos de arboles de decision, junto con la idea del ensamblaje de modelos *boosting* y *bagging* (agregacion boostrap)
 
 - Ilustraremos estos modelos con el analsis de un conjunto de datos de viviendas en Boston
 
 - Usaremos algunas librerias en `R`:
```
install.packages("data.table")
install.packages("gbm")
install.packages("ggplot2")
install.packages("randomForest")
install.packages("tree")
```
En particular, las librerias `tree` y `randomForest` implementan arboles de decision.

-------------------------------------------------------------

-------------------------------------------------------------

# Ilustracion

```{r message=FALSE, include=FALSE}
rm(list=ls())

require("data.table")
require("gbm")
require("ggplot2")
require("randomForest")
require("tree")

set.seed(13)
```

Los datos de viviendas (**Boston Housing**) estan en el archivo `BostonHousing.csv` (vea la [Liga](https://archive.ics.uci.edu/ml/datasets/housing)). Cargamos los datos en formato `data.table` en lugar de `data.frame` (esto no es necesario en este contexto, aunque usuamente facilita la manipulacion de datos *grandes*).

```{r results='hold'}
# Datos
boston_housing <- fread('BostonHousing.csv')
head(boston_housing)
```

Los datos continen la siguiente informacion para **506** observaciones (pueblos):

1. `crim` -  tasa de criminalidad *per capita*

2. `zn` - proporcion de territorio residencial

3. `indus` - Proporcion de acres no residenciales

4. `chas` - cercania con el rio Charles (dicotomica,  1 - si; 0 - no)

5. `nox` - concentracion de oxido nitrico

6. `rm` - numero promedio de habitaciones por vivienda

7. `age` - Proporcion de viviendas constridas antes de 1940

8. `dis` - distancia ponderada a cinco zonas laborales en Boston

9. `rad` - indice de conectividad con autopistas

10. `tax` - taza de impuesto predial

11. `ptratio` - cociente de estudiantes por profesor

12. `b` - indice de proporcion de habitantes de raza negra

13. `lstat` - porcentaje de ingresos bajos en la poblacion

14. `medv` - mediana del valor de las viviendas ocupadas por duenos

Ordenamos los datos por el campo `lstat` y definimos esta variable como la `llave` (o `key`) de los datos:

```{r}
setkey(boston_housing, lstat)
nb_samples <- nrow(boston_housing)
class(boston_housing)
```

# Prediccion de precio de las viviendas

Ilustraremos el uso de los modelos de arboles de decision para predecir el precio de las viviendas (mediana _medv_) con base en la informacion de la concentracion de personas con bajos ingresos (*lstat*). 

Graficamos la asociacion entre estas variables:

```{r fig.width=8, fig.heigth=6}
plot_boston_housing_data <- function(boston_housing_data,
                                     title='S20: medv vs. lstat',
                                     plot_predicted=TRUE) {
  g <- ggplot(boston_housing_data) +
    geom_point(aes(x=lstat, y=medv, color='actual'), size=2) +
    ggtitle(title) +
    xlab('medv') + ylab('lstat')
  
  if (plot_predicted) {
    g <- g +
      geom_line(aes(x=lstat, y=predicted_medv, color='predicted'), size=0.6) +
      scale_colour_manual(name='medv',
                          values=c(actual='blue', predicted='darkorange'))
  } else {
    g <- g +
      scale_colour_manual(name='medv',
                          values=c(actual='blue'))
  }
  
  g <- g +
    theme(plot.title=element_text(face='bold', size=24),
        axis.title=element_text(face='italic', size=18))
  
  g
}
```

Aun no tenemos un modelo, entonces no podemos generar la grafica predictiva:
```{r}
plot_boston_housing_data(boston_housing, plot_predicted=F)
```
```
plot_boston_housing_data(boston_housing, plot_predicted=T)
```
## A. Arbol de decision sencillo

Exploraremos diferentes especificaciones de `arboles de decision`. Empezamos con una parametrizacion basica, con `mincut` siendo el *numero de nodos admisibles*, `minsize` siendo el numero de observaciones admitida en casa nodo, y `midev` siendo la evianza ganada por cada ramificacion creada del arbol. 

El modelo se ajusta con la funcion `tree`:

```{r}
mincut <- 100 
minsize <- 200
mindev <- 1e-6

tree_model <- tree(medv ~ lstat, data=boston_housing,
                   mincut=mincut, minsize=minsize, mindev=mindev)

boston_housing[, predicted_medv := predict(tree_model,
                                           newdata=boston_housing)]

plot_boston_housing_data(boston_housing)
```

La represenyacion simple del arbol de decision arroja una descripcion simple de los datos, con `sesgos` altos y relativamente baja `dispersion`.

## B. Arbolde decision complejo

Definimos un arbolde decision mas complejo ahora:

```{r}
mincut <- 5
minsize <- 10

tree_model_2 <- tree(medv ~ lstat, data=boston_housing,
                   mincut=mincut, minsize=minsize, mindev=mindev)

boston_housing[, predicted_medv := predict(tree_model_2,
                                           newdata=boston_housing)]

plot_boston_housing_data(boston_housing)
```

En este casos los resitados arrojan menores `sesgos` y moderadas con el costo de una `dispersion` mas alta.

#### Practica

Como hemos visto, no es computacionalmente dificil ajustar un `arbol de decision` individual. Sin embargo, vemos tambien que los resultados pueden ser altamente sensibles a su especificacion.

Dado el relativo bajo costo computacional de ajustar un modelo individual, en la **practica** es usual ajustar arboles de decision en la forma de **ensamblaje de modelos**. En particular:

* *Bagging* (la cual es una agregacion tipo _bootstrap_)

* *Boosting*


## C. Bagging and Random Forests

La idea detras del metodo *bagging* es la de ensamblar una coleccion particular de modelos empleando remuestreo. 

* Asi, consideremos $B$ del  mismo tipo (usualmente, se consideran modelos de arboles e decision, pero el metodo se puede aplciar a cualquier clase de modelos) que han sido ajustados (bajo el enfoque bayesiano como frecuentista) con $B$ muestras "bootstrap" (i.e. consideramos el mismo modelos $p(x|\theta)$ el cual es sometrido al mismo procedimiento de aprendizaje para $B$ muestras bootstrap). 

* El procedimiento de remuestreo se implementa al rededor de la muestra de entrenamiento.

* El procedimiento es tipicoamente **predicitivo**, siendo la prediccion final el "promedio" de las prediccion de los $B$ modelos aprendidos. El promedio de tales modelos se conoce como **ensamblaje**. 

**Consideraciones practicas:**

a) Cada **modelo individual** debe ser, **suficientemente complejo** e **insesgdado**, por lo que puede tener una gran *incertidumbre epistemica*

b) Asi, el **ensamblaje model that also has low bias**; and
- In order to make the ensemble model also have low variance, we select a large enough number $B$, so that individual models' high variances offset each other in the aggregate!

The application of "bagging" using tree models is called **Random Forest**:

```{r message=FALSE}
B <- 5000   # number of trees in the Random Forest

rf_model <- randomForest(medv ~ lstat, data=boston_housing,
                         ntree=B,     # number of trees in the Random Forest
                         nodesize=25, # minimum node size set small enough to allow for complex trees,
                                      # but not so small as to require too large B to eliminate high variance
                         keep.inbag=TRUE)

boston_housing[, predicted_medv := predict(rf_model, newdata=boston_housing)]

plot_boston_housing_data(boston_housing)
```

We can see that the average prediction of `r B` trees in the Random Forest, though not a totally smooth function, does well in capturing the signal in the data. We'd be reasonably happy with such a predictive model. The estimated Out-of-Bag (OOB) RMSE of this model is **`r formatC(sqrt(mean(rf_model$mse)), format='f', digits=3)`**.


## D. Boosting

With the [**boosting**](http://en.wikipedia.org/wiki/Boosting_(machine_learning)) method, we successively advance towards a good model fit by adding up small fractions / proportions of relatively simple models that have low variances but possibly high biases. The key intuition is as follows:

- Because individual models are simple and have low variance, the combined additive ensemble model is also likely to have low variance; and
- Because models are successively fit to capture the residuals left over from the previous models, models' biases are likely to offset each other, resulting in an additive ensemble model with low bias!

Let's now build a tree ensemble using boosting:

```{r message=FALSE}
# *** IMPORTANT NOTE ***
# By right, we should be able to easily estimate the OOS RMSE from the Boosted Model
# using GBM's own built-in Cross Validation procedure.
# However, there is currently a BUG that prevents us from using GBM Cross Validation
# in univariate cases.
# Hence, below, we manually produce a training set and a test set to estimate OOS RMSE

train_indices <- sort(sample.int(nb_samples, round(.8 * nb_samples)))
boston_housing_train <- boston_housing[train_indices, ]
boston_housing_test <- boston_housing[-train_indices, ]

boost_model <- gbm(medv ~ lstat, data=boston_housing_train, distribution='gaussian',
                   n.trees=B,           # number of trees
                   interaction.depth=5, # max tree depth
                   n.minobsinnode=40,   # minimum node size, here set higher to avoid high variance
                   shrinkage=0.001,     # shrinkage term, or procedure's "learning rate"
                   bag.fraction=1.,
                   train.fraction=1.)

predicted_medv_test = predict(boost_model, newdata=boston_housing_test, n.trees=B)
oos_rmse = sqrt(mean((predicted_medv_test - boston_housing_test$medv) ^ 2))

# *** IMPORTANT NOTE ***
# The commented-out code below is an alterative GBM function call that, by right,
# should produce the same results; however, there seems to be a BUG with this GBM.FIT function
# resulting in different predictions on test data. DON'T use GBM.FIT.
#
# boost_model <- gbm.fit(x=boston_housing[, .(lstat)], y=boston_housing$medv, distribution='gaussian',
#                        n.trees=B,           
#                        interaction.depth=5, 
#                        n.minobsinnode=40,  
#                        shrinkage=0.001)

boston_housing[, predicted_medv := predict(boost_model, newdata=boston_housing, n.trees=B)]

plot_boston_housing_data(boston_housing)
```

This Boosted Trees ensemble model also looks sound, and has an estimated OOS RSME of **`r formatC(oos_rmse, format='f', digits=3)`**.


----------------------------------

# Tarea

Multivariate Models

Besides their computation inexpensiveness, another _huge_ advantage of using trees-based algorithms is that they are very scalable to multivariate models, and deals with variable interactions very nicely without the need of standard scaling.

Let's build multivariate Random Forest and Boosted Trees models to predict _medv_ using all other variables in the Boston Housing data set:

```{r}
boston_housing[, predicted_medv := NULL]   # remove prediction results column
boston_housing <- boston_housing[sample.int(nb_samples), ]   # shuffle data for more accurate Cross Validation

B <- 10000

rf_model <- randomForest(medv ~ ., data=boston_housing,
                         ntree=B,     # number of trees in the Random Forest
                         nodesize=25, # minimum node size set small enough to allow for complex trees,
                                      # but not so small as to require too large B to eliminate high variance
                         importance=TRUE,
                         keep.inbag=TRUE)

boost_model <- gbm(medv ~ ., data=boston_housing, distribution='gaussian',
                   n.trees=B,            # number of trees
                   interaction.depth=10, # max tree depth
                   n.minobsinnode=40,    # minimum node size, here set higher to avoid high variance
                   shrinkage=0.01,       # shrinkage term, or procedure's "learning rate"
                   bag.fraction=1.,
                   train.fraction=1.,
                   cv.folds=10)          # 10-fold Cross Validation to estimate OOS RMSE
```

The multivariate Random Forest has an estimated OOB RMSE of **`r formatC(sqrt(mean(rf_model$mse)), format='f', digits=3)`**, clearly better than that of the univariate Random Forest model.

The multivariate Boosted Trees model has an estimated Cross Validation-estimated OOS RMSE of **`r  formatC(sqrt(mean(boost_model$cv.error)), format='f', digits=3)`**, again a clear improvement from the univariate Boosted Trees model.

We can also see that the two models' prediction are pretty aligned:

```{r}
plot(rf_model$predicted, boost_model$fit)
title('Random Forest vs. Boosted Trees predictions')
```


The two models also agree on the most important variables:

```{r}
varImpPlot(rf_model, main="Random Forest's Variable Importance")
plot(summary(boost_model, plotit=FALSE), main="Boosted Trees Model's Variable Importance")
```
